# Image to Caption to Image
December 2016 - This project and the project materials are built with 3 other excellent teammates; Safa Messaoud, Ankit Rai and Tarek Elgamal.

### Summary
In this project, we built a pipeline that consists of two models: The first model, Show and Tell (Im2Txt) [1], generates a caption from a given image. The second model, Txt2Im [2], produces an image from a given caption. In addition to training and implementing these two models independently, by combining them, we were also able to compare a given image with the pipeline-generated one and comment on the information conveyed by a caption of just a few words. 

*Obviously, it is not expected to obtain a perfect match between these two images. If not impossible, it would take much more computational power, data and manpower for exquisite modeling than available to us. However, the futuristic idea of perfect reconstruction is very exciting. Imagine storing images as text, taking up a tiny fraction of memory or bandwidth, and then generating them from those "captions" as necessary.*

Details of the project can be found [in this presentation](Show_and_Tell_Presentation.pdf) and [in this report](Show_and_Tell_Project_report.pdf)

### (Some) Results
Some captions generated by im2txt model trained on Flickr8k dataset: Top - captions with high confidence scores (good captions), bottom - captions with low confidence scores (bad captions)
<p align="center">
  <img src="/Results/good_captions.png" width="600" title="Good captions">
  <img src="/Results/bad_captions.png" width="600" title="Bad captions">
</p>

Some example results of the pipeline. Left images are the ones fed into the trained im2txt network, middle text is the captions generated by im2txt, and on the right are the images generated by the (not-so-well) trained txt2im network.

Although the networks are trained on a relatively small dataset (Flicker8k with 8k images) for a relatively low amount of time (especially the generative network, which is trained for just 6 epochs), the results are somehow related. The generative network seem to get the color cues. In the first example, the generated images have more blue background, which probably corresponds to "water" in the caption, with dark spots in the center, which probably corresponds to "brown dog". In the second, similarly, most of the generated images has greenish background with dark blobs around the center. 
<p align="center">
  <img src="/Results/im2txt2im_1.png" width="600">
  <img src="/Results/im2txt2im_2.png" width="600">
</p>

### References 
1. Vinyals, O., Toshev, A., Bengio, S. and Erhan, D., 2015. Show and Tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3156-3164).
2. Mansimov, E., Parisotto, E., Ba, J.L. and Salakhutdinov, R., 2015. Generating images from captions with attention. arXiv:1511.02793.
3. Chen, Xinlei and Zitnick, C Lawrence. Learning a recurrent visual representation for image caption generation. arXiv:1411.5654, 2014.
4. Bahdanau et al. ”Neural machine translation by jointly learning to align and translate.” arXiv:1409.0473 (2014).
5. Xu, Kelvin, et al.”Show, attend and tell:Neural image caption generation with visual attention.” arXiv:1502.03044 2.3 (2015): 5.
6. Andrej Karpathy, Li Fei-Fei. ”Deep Visual-Semantic Alignments for Generating Image Descriptions.” CVPR 2015.

### Baseline Code
1. https://github.com/tensorflow/models/tree/master/im2txt
2. https://github.com/emansim/text2image
